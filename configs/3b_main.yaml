# Accelerate configuration for distributed training across 3 nodes
# This config sets up training across 3 machines with multiple GPUs per machine

compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0  # Change this to 0, 1, or 2 for each node respectively
main_process_ip: 10.1.10.51  # Replace with your main node's IP address
main_process_port: 29500
main_training_function: main
mixed_precision: bf16  # Can be 'no', 'fp16', or 'bf16'
num_machines: 3  # Total number of nodes
num_processes: 24  # Total processes across all nodes (e.g., 8 GPUs per node Ã— 3 nodes)
rdzv_backend: static  # Can also use 'c10d' for dynamic node discovery
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false

# Additional settings for multi-node training
deepspeed_config: {}  # Add DeepSpeed config here if needed 